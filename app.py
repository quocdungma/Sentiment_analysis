
import random
import streamlit as st
import requests
import pandas as pd
import json
import pickle
import regex
from underthesea import word_tokenize, pos_tag, sent_tokenize
from pyvi import ViTokenizer
import re
import sklearn
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import hstack
import numpy as np
import joblib

# Load the trained model and vectorizer
vectorizer = joblib.load('tfidf_vectorizer.pkl')
model = joblib.load('random_forest_model.pkl')

# Define the URL of the API
url = "https://tiki.vn/api/v2/products?q="
search_query = "may tinh bang"
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36'
}

def process_text(text, emoji_dict, teen_dict, wrong_lst):
    document = text.lower()
    document = document.replace("â€™",'')
    document = regex.sub(r'\.+', ".", document)
    new_sentence =''
    for sentence in sent_tokenize(document):
        # if not(sentence.isascii()):
        ###### CONVERT EMOJICON
        sentence = ''.join(emoji_dict[word]+' ' if word in emoji_dict else word for word in list(sentence))
        ###### CONVERT TEENCODE
        sentence = ' '.join(teen_dict[word] if word in teen_dict else word for word in sentence.split())
        ###### DEL Punctuation & Numbers
        pattern = r'(?i)\b[a-zÃ¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã­Ã¬á»‰Ä©á»‹ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄ‘]+\b'
        sentence = ' '.join(regex.findall(pattern,sentence))
        # ...
        ###### DEL wrong words
        sentence = ' '.join('' if word in wrong_lst else word for word in sentence.split())
        new_sentence = new_sentence+ sentence + '. '
    document = new_sentence
    #print(document)
    ###### DEL excess blank space
    document = regex.sub(r'\s+', ' ', document).strip()
    #...
    return document

# Chuáº©n hÃ³a unicode tiáº¿ng viá»‡t
def loaddicchar():
    uniChars = "Ã Ã¡áº£Ã£áº¡Ã¢áº§áº¥áº©áº«áº­Äƒáº±áº¯áº³áºµáº·Ã¨Ã©áº»áº½áº¹Ãªá»áº¿á»ƒá»…á»‡Ä‘Ã¬Ã­á»‰Ä©á»‹Ã²Ã³á»Ãµá»Ã´á»“á»‘á»•á»—á»™Æ¡á»á»›á»Ÿá»¡á»£Ã¹Ãºá»§Å©á»¥Æ°á»«á»©á»­á»¯á»±á»³Ã½á»·á»¹á»µÃ€Ãáº¢Ãƒáº Ã‚áº¦áº¤áº¨áºªáº¬Ä‚áº°áº®áº²áº´áº¶ÃˆÃ‰áººáº¼áº¸ÃŠá»€áº¾á»‚á»„á»†ÄÃŒÃá»ˆÄ¨á»ŠÃ’Ã“á»Ã•á»ŒÃ”á»’á»á»”á»–á»˜Æ á»œá»šá»á» á»¢Ã™Ãšá»¦Å¨á»¤Æ¯á»ªá»¨á»¬á»®á»°á»²Ãá»¶á»¸á»´Ã‚Ä‚ÄÃ”Æ Æ¯"
    unsignChars = "aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU"

    dic = {}
    char1252 = 'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£'.split(
        '|')
    charutf8 = "Ã |Ã¡|áº£|Ã£|áº¡|áº§|áº¥|áº©|áº«|áº­|áº±|áº¯|áº³|áºµ|áº·|Ã¨|Ã©|áº»|áº½|áº¹|á»|áº¿|á»ƒ|á»…|á»‡|Ã¬|Ã­|á»‰|Ä©|á»‹|Ã²|Ã³|á»|Ãµ|á»|á»“|á»‘|á»•|á»—|á»™|á»|á»›|á»Ÿ|á»¡|á»£|Ã¹|Ãº|á»§|Å©|á»¥|á»«|á»©|á»­|á»¯|á»±|á»³|Ã½|á»·|á»¹|á»µ|Ã€|Ã|áº¢|Ãƒ|áº |áº¦|áº¤|áº¨|áºª|áº¬|áº°|áº®|áº²|áº´|áº¶|Ãˆ|Ã‰|áºº|áº¼|áº¸|á»€|áº¾|á»‚|á»„|á»†|ÃŒ|Ã|á»ˆ|Ä¨|á»Š|Ã’|Ã“|á»|Ã•|á»Œ|á»’|á»|á»”|á»–|á»˜|á»œ|á»š|á»|á» |á»¢|Ã™|Ãš|á»¦|Å¨|á»¤|á»ª|á»¨|á»¬|á»®|á»°|á»²|Ã|á»¶|á»¸|á»´".split(
        '|')
    for i in range(len(char1252)):
        dic[char1252[i]] = charutf8[i]
    return dic

# ÄÆ°a toÃ n bá»™ dá»¯ liá»‡u qua hÃ m nÃ y Ä‘á»ƒ chuáº©n hÃ³a láº¡i
def covert_unicode(txt):
    dicchar = loaddicchar()
    return regex.sub(
        r'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£',
        lambda x: dicchar[x.group()], txt)

def find_words(document, list_of_words):
    document_lower = document.lower()
    word_count = 0
    word_list = []

    for word in list_of_words:
        if word in document_lower:
            word_count += document_lower.count(word)
            word_list.append(word)

    return word_count, word_list

def remove_stopword(text, stopwords):
    ###### REMOVE stop words
    document = ' '.join('' if word in stopwords else word for word in text.split())
    #print(document)
    ###### DEL excess blank space
    document = regex.sub(r'\s+', ' ', document).strip()
    return document

##LOAD EMOJICON
file = open('files/emojicon.txt', 'r', encoding="utf8")
emoji_lst = file.read().split('\n')
emoji_dict = {}
for line in emoji_lst:
    key, value = line.split('\t')
    emoji_dict[key] = str(value)
file.close()
#################
#LOAD TEENCODE
file = open('files/teencode.txt', 'r', encoding="utf8")
teen_lst = file.read().split('\n')
teen_dict = {}
for line in teen_lst:
    key, value = line.split('\t')
    teen_dict[key] = str(value)
file.close()
###############
#LOAD TRANSLATE ENGLISH -> VNMESE
file = open('files/english-vnmese.txt', 'r', encoding="utf8")
english_lst = file.read().split('\n')
english_dict = {}
for line in english_lst:
    key, value = line.split('\t')
    english_dict[key] = str(value)
file.close()
################
#LOAD wrong words
file = open('files/wrong-word.txt', 'r', encoding="utf8")
wrong_lst = file.read().split('\n')
file.close()
#################
#LOAD STOPWORDS
file = open('files/vietnamese-stopwords.txt', 'r', encoding="utf8")
stopwords_lst = file.read().split('\n')
file.close()

neutral_words = ["cháº¥p nháº­n Ä‘Æ°á»£c", "trung bÃ¬nh", "bÃ¬nh thÆ°á»ng", "táº¡m á»•n", "trung láº­p", "cÃ³ thá»ƒ"
                 "khÃ´ng ná»•i báº­t", "Ä‘á»§ á»•n", "Ä‘á»§ tá»‘t", "cÃ³ thá»ƒ cháº¥p nháº­n", "bÃ¬nh thÆ°á»ng",
                 "thÆ°á»ng xuyÃªn", "tÆ°Æ¡ng Ä‘á»‘i", "há»£p lÃ½", "tÆ°Æ¡ng tá»±",
                 "cÃ³ thá»ƒ sá»­ dá»¥ng", "bÃ¬nh yÃªn", "bÃ¬nh tÄ©nh", "khÃ´ng quÃ¡ tá»‡", "trung háº¡ng",
                 "cÃ³ thá»ƒ Ä‘iá»ƒm cá»™ng", "dá»… cháº¥p nháº­n", "khÃ´ng pháº£i lÃ  váº¥n Ä‘á»",
                 "khÃ´ng pháº£n Ä‘á»‘i", "khÃ´ng quÃ¡ Ä‘Ã¡ng ká»ƒ", "khÃ´ng gÃ¢y báº¥t ngá»", "khÃ´ng táº¡o áº¥n tÆ°á»£ng", "cÃ³ thá»ƒ cháº¥p nháº­n",
                 "khÃ´ng gÃ¢y sá»‘c", "tÆ°Æ¡ng Ä‘á»‘i tá»‘t", "khÃ´ng thay Ä‘á»•i", "khÃ´ng quÃ¡ phá»©c táº¡p", "khÃ´ng Ä‘Ã¡ng ká»ƒ",
                 "cháº¥p nháº­n", "cÃ³ thá»ƒ dá»… dÃ ng thÃ­ch nghi", "khÃ´ng quÃ¡ cáº§u ká»³", "khÃ´ng cáº§n thiáº¿t", "khÃ´ng yÃªu cáº§u nhiá»u", "khÃ´ng gÃ¢y háº¡i",
                 "khÃ´ng cÃ³ sá»± thay Ä‘á»•i Ä‘Ã¡ng ká»ƒ", "khÃ´ng rÃµ rÃ ng", "khÃ´ng quÃ¡ phÃª bÃ¬nh", "khÃ´ng Ä‘Ã¡ng chÃº Ã½", "khÃ´ng Ä‘áº·c biá»‡t",
                 "khÃ´ng quÃ¡ phá»©c táº¡p", "khÃ´ng gÃ¢y phiá»n hÃ ", "khÃ´ng Ä‘Ã¡ng ká»ƒ", "khÃ´ng gÃ¢y kÃ­ch thÃ­ch"]

negative_words = [
    "kÃ©m", "tá»‡", "Ä‘au", "xáº¥u", "bá»‹","rÃ¨", "á»“n",
    "buá»“n", "rá»‘i", "thÃ´", "lÃ¢u", "sai", "hÆ°", "dÆ¡", "khÃ´ng cÃ³"
    "tá»‘i", "chÃ¡n", "Ã­t", "má»", "má»ng", "vá»¡", "hÆ° há»ng",
    "lá»ng láº»o", "khÃ³", "cÃ¹i", "yáº¿u", "mÃ ", "khÃ´ng thÃ­ch", "khÃ´ng thÃº vá»‹", "khÃ´ng á»•n",
    "khÃ´ng há»£p", "khÃ´ng Ä‘Ã¡ng tin cáº­y", "khÃ´ng chuyÃªn nghiá»‡p", "nháº§m láº«n"
    "khÃ´ng pháº£n há»“i", "khÃ´ng an toÃ n", "khÃ´ng phÃ¹ há»£p", "khÃ´ng thÃ¢n thiá»‡n", "khÃ´ng linh hoáº¡t", "khÃ´ng Ä‘Ã¡ng giÃ¡",
    "khÃ´ng áº¥n tÆ°á»£ng", "khÃ´ng tá»‘t", "cháº­m", "khÃ³ khÄƒn", "phá»©c táº¡p", "bá»‹ má»Ÿ", "bá»‹ khui", "khÃ´ng Ä‘Ãºng", "khÃ´ng Ä‘Ãºng sáº£n pháº©m",
    "khÃ³ hiá»ƒu", "khÃ³ chá»‹u", "gÃ¢y khÃ³ dá»…", "rÆ°á»m rÃ ", "khÃ³ truy cáº­p", "bá»‹ bÃ³c", "sai sáº£n pháº©m",
    "tháº¥t báº¡i", "tá»“i tá»‡", "khÃ³ xá»­", "khÃ´ng thá»ƒ cháº¥p nháº­n", "tá»“i tá»‡","khÃ´ng rÃµ rÃ ng", "giáº£m cháº¥t lÆ°á»£ng",
    "khÃ´ng cháº¯c cháº¯n", "rá»‘i ráº¯m", "khÃ´ng tiá»‡n lá»£i", "khÃ´ng Ä‘Ã¡ng tiá»n", "chÆ°a Ä‘áº¹p", "khÃ´ng Ä‘áº¹p"
]

positive_words = [
    "thÃ­ch", "tá»‘t", "xuáº¥t sáº¯c","Ä‘Ãºng", "tuyá»‡t vá»i", "tuyá»‡t háº£o", "Ä‘áº¹p", "á»•n"
    "hÃ i lÃ²ng", "Æ°ng Ã½", "hoÃ n háº£o", "cháº¥t lÆ°á»£ng", "thÃº vá»‹", "nhanh"
    "tiá»‡n lá»£i", "dá»… sá»­ dá»¥ng", "hiá»‡u quáº£", "áº¥n tÆ°á»£ng",
    "ná»•i báº­t", "táº­n hÆ°á»Ÿng", "tá»‘n Ã­t thá»i gian", "thÃ¢n thiá»‡n", "háº¥p dáº«n",
    "gá»£i cáº£m", "tÆ°Æ¡i má»›i", "láº¡ máº¯t", "cao cáº¥p", "Ä‘á»™c Ä‘Ã¡o",
    "há»£p kháº©u vá»‹", "ráº¥t tá»‘t", "ráº¥t thÃ­ch", "táº­n tÃ¢m", "Ä‘Ã¡ng tin cáº­y", "Ä‘áº³ng cáº¥p",
    "háº¥p dáº«n", "an tÃ¢m", "khÃ´ng thá»ƒ cÆ°á»¡ng_láº¡i", "thá»a mÃ£n", "thÃºc Ä‘áº©y",
    "cáº£m Ä‘á»™ng", "phá»¥c vá»¥ tá»‘t", "lÃ m hÃ i lÃ²ng", "gÃ¢y áº¥n tÆ°á»£ng", "ná»•i trá»™i",
    "sÃ¡ng táº¡o", "quÃ½ bÃ¡u", "phÃ¹ há»£p", "táº­n tÃ¢m",
    "hiáº¿m cÃ³", "cáº£i thiá»‡n", "hoÃ  nhÃ£", "chÄƒm chá»‰", "cáº©n tháº­n",
    "vui váº»", "sÃ¡ng sá»§a", "hÃ o há»©ng", "Ä‘am mÃª", "vá»«a váº·n", "Ä‘Ã¡ng tiá»n"
]

# Danh sÃ¡ch cÃ¡c tá»« mang Ã½ nghÄ©a phá»§ Ä‘á»‹nh
negation_words = ["khÃ´ng", "nhÆ°ng", "tuy nhiÃªn", "máº·c dÃ¹", "cháº³ng", "mÃ ", 'kÃ©m', 'giáº£m']

positive_emojis = [
    "ğŸ˜„", "ğŸ˜ƒ", "ğŸ˜€", "ğŸ˜", "ğŸ˜†",
    "ğŸ˜…", "ğŸ¤£", "ğŸ˜‚", "ğŸ™‚", "ğŸ™ƒ",
    "ğŸ˜‰", "ğŸ˜Š", "ğŸ˜‡", "ğŸ¥°", "ğŸ˜",
    "ğŸ¤©", "ğŸ˜˜", "ğŸ˜—", "ğŸ˜š", "ğŸ˜™",
    "ğŸ˜‹", "ğŸ˜›", "ğŸ˜œ", "ğŸ¤ª", "ğŸ˜",
    "ğŸ¤—", "ğŸ¤­", "ğŸ¥³", "ğŸ˜Œ", "ğŸ˜",
    "ğŸ¤“", "ğŸ§", "ğŸ‘", "ğŸ¤", "ğŸ™Œ", "ğŸ‘", "ğŸ‘‹",
    "ğŸ¤™", "âœ‹", "ğŸ–ï¸", "ğŸ‘Œ", "ğŸ¤",
    "âœŒï¸", "ğŸ¤Ÿ", "ğŸ‘ˆ", "ğŸ‘‰", "ğŸ‘†",
    "ğŸ‘‡", "â˜ï¸"
]

# Count emojis positive and negative
negative_emojis = [
    "ğŸ˜", "ğŸ˜”", "ğŸ™", "â˜¹ï¸", "ğŸ˜•",
    "ğŸ˜¢", "ğŸ˜­", "ğŸ˜–", "ğŸ˜£", "ğŸ˜©",
    "ğŸ˜ ", "ğŸ˜¡", "ğŸ¤¬", "ğŸ˜¤", "ğŸ˜°",
    "ğŸ˜¨", "ğŸ˜±", "ğŸ˜ª", "ğŸ˜“", "ğŸ¥º",
    "ğŸ˜’", "ğŸ™„", "ğŸ˜‘", "ğŸ˜¬", "ğŸ˜¶",
    "ğŸ¤¯", "ğŸ˜³", "ğŸ¤¢", "ğŸ¤®", "ğŸ¤•",
    "ğŸ¥´", "ğŸ¤”", "ğŸ˜·", "ğŸ™…â€â™‚ï¸", "ğŸ™…â€â™€ï¸",
    "ğŸ™†â€â™‚ï¸", "ğŸ™†â€â™€ï¸", "ğŸ™‡â€â™‚ï¸", "ğŸ™‡â€â™€ï¸", "ğŸ¤¦â€â™‚ï¸",
    "ğŸ¤¦â€â™€ï¸", "ğŸ¤·â€â™‚ï¸", "ğŸ¤·â€â™€ï¸", "ğŸ¤¢", "ğŸ¤§",
    "ğŸ¤¨", "ğŸ¤«", "ğŸ‘", "ğŸ‘Š", "âœŠ", "ğŸ¤›", "ğŸ¤œ",
    "ğŸ¤š", "ğŸ–•"
]

# Äá»‹nh nghÄ©a hÃ m tiá»n xá»­ lÃ½
def preprocess_input(input_text, emoji_dict, teen_dict, wrong_lst, neutral_words, negative_words, positive_words, negation_words, positive_emojis, negative_emojis, stopwords_lst):
    # BÆ°á»›c 1: Ãp dá»¥ng hÃ m xá»­ lÃ½ vÄƒn báº£n
    processed_text = process_text(input_text, emoji_dict, teen_dict, wrong_lst)

    # BÆ°á»›c 2: Chuyá»ƒn Ä‘á»•i kÃ½ tá»± unicode
    processed_text = covert_unicode(processed_text)

    # BÆ°á»›c 3: TÃ­nh toÃ¡n sá»‘ lÆ°á»£ng tá»« vÃ  emoji mang tÃ­nh cáº£m xÃºc
    neutral_word_count = find_words(processed_text, neutral_words)[0]
    negative_word_count = find_words(processed_text, negative_words)[0]
    positive_word_count = max(find_words(processed_text, positive_words)[0] - find_words(processed_text, negation_words)[0],0)
    positive_emoji_count = find_words(processed_text, positive_emojis)[0]
    negative_emoji_count = find_words(processed_text, negative_emojis)[0]

    # BÆ°á»›c 4: TÃ¡ch tá»« vÃ  loáº¡i bá» stopwords
    tokenized_text = word_tokenize(processed_text, format="text")
    tokenized_text = remove_stopword(tokenized_text, stopwords_lst)

    # BÆ°á»›c 5: Ãp dá»¥ng POS tagging
    tokenized_text = ViTokenizer.tokenize(tokenized_text)
    tokenized_text = re.sub(r'\.', '', tokenized_text)

    # Gom cÃ¡c thÃ´ng tin Ä‘Ã£ xá»­ lÃ½ vÃ o má»™t dictionary
    processed_data = {
        "processed_text": tokenized_text,
        "neutral_word_count": neutral_word_count,
        "negative_word_count": negative_word_count,
        "positive_word_count": positive_word_count,
        "positive_emoji_count": positive_emoji_count,
        "negative_emoji_count": negative_emoji_count
    }

    return processed_data

def predict_sentiment(user_input):
    # BÆ°á»›c 1: Tiá»n xá»­ lÃ½ Ä‘áº§u vÃ o tá»« ngÆ°á»i dÃ¹ng
    processed_data = preprocess_input(user_input, emoji_dict, teen_dict, wrong_lst, neutral_words, negative_words, positive_words, negation_words, positive_emojis, negative_emojis, stopwords_lst)

    # BÆ°á»›c 2: TrÃ­ch xuáº¥t cÃ¡c Ä‘áº·c trÆ°ng
    processed_text = processed_data['processed_text']
    feature_values = [
        processed_data['neutral_word_count'],
        processed_data['negative_word_count'],
        processed_data['positive_word_count'],
        processed_data['positive_emoji_count'],
        processed_data['negative_emoji_count']
    ]

    # BÆ°á»›c 3: Vector hÃ³a vÄƒn báº£n Ä‘Ã£ Ä‘Æ°á»£c tiá»n xá»­ lÃ½
    text_vectorized = vectorizer.transform([processed_text])

    # BÆ°á»›c 4: Káº¿t há»£p Ä‘áº·c trÆ°ng vÄƒn báº£n vá»›i cÃ¡c Ä‘áº·c trÆ°ng bá»• sung
    features_combined = hstack((text_vectorized, np.array(feature_values).reshape(1, -1)))

    # BÆ°á»›c 5: Viáº¿t hÃ m dá»± bÃ¡o
    prediction = model.predict(features_combined)

    return prediction[0]

# Example usage:
# comment = "sáº£n pháº©m khÃ´ng cháº¥t lÆ°á»£ng"

# prediction = predict_sentiment(comment)
# print(f"The predicted sentiment is: {prediction}")

# ThÃªm sidebar
st.sidebar.header('Sentiment analysis',divider='rainbow')

sidebar_option = st.sidebar.selectbox(
    "Lá»±a chá»n ná»™i dung",
    ("Giá»›i thiá»‡u", "ğŸ” TÃ¬m kiáº¿m theo sáº£n pháº©m", "Nháº­p bÃ¬nh luáº­n Ä‘á»ƒ dá»± Ä‘oÃ¡n")
)

st.sidebar.markdown(f"<h1 style='font-size:10px;'>ğŸ˜Š TÃ­ch cá»±c</h1>", unsafe_allow_html=True)
st.sidebar.markdown(f"<h1 style='font-size:10px;'>ğŸ˜ Trung tÃ­nh</h1>", unsafe_allow_html=True)
st.sidebar.markdown(f"<h1 style='font-size:10px;'>ğŸ˜¢ TiÃªu cá»±c</h1>", unsafe_allow_html=True)

if sidebar_option == "Giá»›i thiá»‡u":
    st.title("Giá»›i thiá»‡u vá» á»©ng dá»¥ng")
    st.header('HÆ°á»›ng dáº«n sá»­ dá»¥ng app:', divider='rainbow')
    st.subheader('1. TÃ¬m kiáº¿m sáº£n pháº©m:', divider='rainbow')
    st.write("   - NgÆ°á»i dÃ¹ng nháº­p vÃ o sáº£n pháº©m cáº§n tÃ¬m.")
    st.write("   - Báº¥m vÃ o tÃªn sáº£n pháº©m Ä‘á»ƒ xem comment.")
    st.write("   - MÃ´ hÃ¬nh Ä‘á»c vÃ  há»— trá»£ dá»± bÃ¡o comment.")
    st.write("   - Äá»‘i chiáº¿u vá»›i comment thá»±c táº¿.")
    st.write("   - TrÃ¡nh trÆ°á»ng há»£p rating vÃ  comment khÃ´ng tÆ°Æ¡ng Ä‘á»“ng.")

    st.subheader('2. Nháº­p bÃ¬nh luáº­n Ä‘á»ƒ dá»± Ä‘oÃ¡n:', divider='rainbow')
    st.write("   - NgÆ°á»i dÃ¹ng nháº­p vÃ o má»™t Ä‘oáº¡n comment.")
    st.write("   - MÃ´ hÃ¬nh Ä‘Ã¡nh giÃ¡ comment Ä‘Ã³ lÃ  tÃ­ch cá»±c, tiÃªu cá»±c, hay trung tÃ­nh.")

    st.header('NgÆ°á»i thá»±c hiá»‡n:', divider='rainbow')
    st.write("MÃ£ Quá»‘c DÅ©ng")
    st.write("Nguyá»…n Thanh Trá»ng")

    st.header('Nguá»“n dá»¯ liá»‡u:', divider='rainbow')
    st.write("Dá»¯ liá»‡u Ä‘Æ°á»£c thu tháº­p tá»« hai nguá»“n chÃ­nh: Tiki vÃ  Sendo.")
    st.write("Dá»¯ liá»‡u tÃ¬m kiáº¿m Ä‘Æ°á»£c cÃ o trá»±c tiáº¿p tá»« trang sáº£n pháº©m cá»§a Tiki.")

    st.header('PhÆ°Æ¡ng phÃ¡p sá»­ dá»¥ng:', divider='rainbow')
    st.write("PhÆ°Æ¡ng phÃ¡p Random Forest")
    st.write("Äá»™ chÃ­nh xÃ¡c: gáº§n 94%")
    st.image("https://editor.analyticsvidhya.com/uploads/74060RF%20image.jpg", caption="MÃ´ hÃ¬nh Random Forest")

elif sidebar_option == "ğŸ” TÃ¬m kiáº¿m theo sáº£n pháº©m":
    st.title("TÃ¬m kiáº¿m nháº­n xÃ©t theo sáº£n pháº©m")

    search_query = st.text_input("Nháº­p thÃ´ng tin:")
    if st.button("TÃ¬m kiáº¿m ğŸ”"):
        st.write(f"Káº¿t quáº£ tÃ¬m kiáº¿m cho: {search_query}")

    # Láº¥y sáº£n pháº©m products
    import requests

    # Define the URL of the API
    url = "https://tiki.vn/api/v2/products?q=" + search_query

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36'
    }
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()  # Raise an exception for unsuccessful HTTP status codes

        data = response.json()

    except requests.exceptions.RequestException as e:
        st.write(f"An error occurred: {e}")
        data = {"data": []}
    except json.JSONDecodeError as e:
        st.write(f"Error decoding JSON: {e}")
        data = {"data": []}

    # Extracting the necessary information and creating a DataFrame
    review_link = 'https://tiki.vn/api/v2/reviews?product_id='
    # Extracting the necessary information and creating a list of dictionaries
    product_list = []
    for item in data["data"]:
        product = {
            "product_id": str(item["id"]),
            "name": item["name"],
            "sold": item.get("quantity_sold", {}).get("value", "N/A"),
            "price": item.get("price", "N/A"),
            "image": item.get("thumbnail_url", "N/A"),
            "review_count": item.get("review_count", "N/A"),
            "rating_average": item.get("rating_average", "N/A"),
            "link_comment": review_link + str(item["id"])
        }
        product_list.append(product)
    products = product_list


    current_product_id = None
    # Láº¥y comments dá»±a vÃ o products vÃ  cá»™t link_comment
    comments = []
    for product in products:
        review_url = product['link_comment']
        try:
            review_response = requests.get(review_url, headers=headers)
            review_response.raise_for_status()  # Raise an exception for unsuccessful HTTP status codes

            review_data = review_response.json()

            comments_productid = [
                {
                    "product_id": product["product_id"],
                    "username": review.get("created_by", {}).get("name", "Anonymous"),
                    "rating": review.get("rating", 0),
                    "content": review.get("content", "No content"),
                }
                for review in review_data.get("data", [])
            ]
            comments.extend(comments_productid)
        except requests.exceptions.RequestException as e:
            st.write(f"An error occurred: {e}")
        except json.JSONDecodeError as e:
            st.write(f"Error decoding JSON: {e}")

    st.session_state.comments = comments


    # Display all the comments or process them as you need
    # for comment in st.session_state.comments:
    #    st.write(f"Product ID: {comment['product_id']}")
    #    st.write(f"Username: {comment['username']}")
    #    st.write(f"Rating: {comment['rating']} stars")
    #    st.write(f"Content: {comment['content']}")

    # Debug: print the number of comments fetched
    st.write(f"Number of comments fetched: {len(comments)}")

    def display_comments(product_id):
        product_comments = [comment for comment in st.session_state.comments if comment["product_id"] == product_id]
        for comment in product_comments:
            st.write(f"TÃªn: {comment['username']}")
            st.write(f"ÄÃ¡nh giÃ¡: {comment['rating']} sao")
            st.write(f"Ná»™i dung: {comment['content']}")

            prediction = predict_sentiment(comment['content'])
            if prediction == 'positive':
                st.markdown(f"<h1 style='font-size:20px;'>Predict: ğŸ˜Š</h1>", unsafe_allow_html=True)
            elif prediction == 'neutral':
                st.markdown(f"<h1 style='font-size:20px;'>Predict: ğŸ˜</h1>", unsafe_allow_html=True)
            else: # Assuming 'negative' is the only other value returned by the predict function
                st.markdown(f"<h1 style='font-size:20px;'>Predict: ğŸ˜¢</h1>", unsafe_allow_html=True)

    cols = st.columns(2)
    for i, product in enumerate(products):
        with cols[i % 2]:
            st.image(product["image"])
            if st.button(product["name"], key=f"product_{product['product_id']}"):
                current_product_id = product["product_id"]
                display_comments(product["product_id"])
            # Äiá»u chá»‰nh cá»¡ chá»¯ vÃ  Ä‘á»‹nh dáº¡ng chá»¯ cho cÃ¡c trÆ°á»ng 'sold' vÃ  'price'
            st.markdown(f"<p style='font-size: small;'>ÄÃ£ bÃ¡n: {product['sold']}</p>", unsafe_allow_html=True)
            st.markdown(f"<p style='font-size: small;'>Reviews: {product['review_count']}</p>", unsafe_allow_html=True)
            st.markdown(f"<p style='font-size: small;'>Rate: {product['rating_average']}</p>", unsafe_allow_html=True)
            st.markdown(f"<p style='font-size: larger; font-weight: bold;'>GiÃ¡: {int(product['price']):,} Ä‘</p>", unsafe_allow_html=True)

elif sidebar_option == "Nháº­p bÃ¬nh luáº­n Ä‘á»ƒ dá»± Ä‘oÃ¡n":
    st.title("ÄÃ¡nh giÃ¡ nháº­n xÃ©t cá»§a ngÆ°á»i dÃ¹ng")
    user_input = st.text_area("Nháº­p bÃ¬nh luáº­n cá»§a báº¡n á»Ÿ Ä‘Ã¢y")
    if st.markdown("""
        <style>
        .custom-button {
            font-size: 20px;
            height: 50px;
            width: 200px;
            border: 2px solid white;
            border-radius: 20px;
            text-align: center;
            line-height: 50px; /* This will vertically center the text */
            padding-left: 1px; /* This will horizontally align the text to the left */
        }
        </style>
        <button class="custom-button" onclick="handleClick()">ğŸ” Dá»± Ä‘oÃ¡n</button>
        <script>
            function handleClick() {
                // Logic to handle button click
            }
        </script>
        """, unsafe_allow_html=True):
        prediction = predict_sentiment(user_input)
        if prediction == 'positive':
            st.markdown(f"<h1 style='font-size:30px;'>Predict: ğŸ˜Š</h1>", unsafe_allow_html=True)
        elif prediction == 'neutral':
            st.markdown(f"<h1 style='font-size:30px;'>Predict: ğŸ˜</h1>", unsafe_allow_html=True)
        else: # Assuming 'negative' is the only other value returned by the predict function
            st.markdown(f"<h1 style='font-size:30px;'>Predict: ğŸ˜¢</h1>", unsafe_allow_html=True)
